{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get subset of ids for JRB basins ####\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "gdf = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"flowpaths\")\n",
    "nexus = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"nexus\")\n",
    "# Many more layers 'flowpaths', 'divides', 'lakes', 'nexus', 'pois', 'hydrolocations', 'flowpath-attributes',\n",
    "# 'flowpath-attributes-ml', 'network', 'divide-attributes'\n",
    "\n",
    "# print(gdf.head())\n",
    "print(f\"Basins in Juniata RB: {gdf.divide_id} (unique: {gdf.divide_id.nunique()})\")\n",
    "\n",
    "# Select subset of divide_ids\n",
    "jrb_divide_ids = list(gdf.divide_id)[0:1]\n",
    "print(f\"selecting divide_id: {jrb_divide_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"network\")\n",
    "# gdf[gdf['divide_id'].isin(jrb_divide_ids)]\n",
    "gdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CRS projection; need to use the 'flowpaths' layer\n",
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf[gdf[\"divide_id\"].isin(jrb_divide_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert catchment data gdf to geojson ####\n",
    "\n",
    "import json\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "filtered_gdf = gdf[gdf[\"divide_id\"].isin(jrb_divide_ids)]\n",
    "\n",
    "# Reproject to WGS84 (4326)\n",
    "filtered_gdf = filtered_gdf.to_crs(\"EPSG:4326\")\n",
    "print(f\"converted CRS -> {filtered_gdf.crs}\\n\")\n",
    "\n",
    "# Ensure the LINESTRING is closed (first and last points are the same)\n",
    "line = filtered_gdf.iloc[0].geometry\n",
    "if line.coords[0] != line.coords[-1]:\n",
    "    line = Polygon(list(line.coords) + [line.coords[0]])\n",
    "\n",
    "# Update the geometry in the GeoDataFrame\n",
    "filtered_gdf.at[filtered_gdf.index[0], \"geometry\"] = line\n",
    "\n",
    "# Create GeoJSON object with structure for ngen.\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"name\": \"catchment_data\",\n",
    "    \"crs\": {\n",
    "        \"type\": \"name\",\n",
    "        \"properties\": {\"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\"},\n",
    "    },\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"id\": filtered_gdf.iloc[0][\"divide_id\"],  # Use divide_id as the feature ID\n",
    "            \"properties\": {\n",
    "                \"area_sqkm\": filtered_gdf.iloc[0][\n",
    "                    \"areasqkm\"\n",
    "                ],  # Area in square kilometers\n",
    "                \"toid\": filtered_gdf.iloc[0][\"toid\"],  # Related identifier\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": [\n",
    "                    list(line.exterior.coords),\n",
    "                ],  # Extract polygon coordinates\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"catchment_data_cat-88306.geojson\", \"w\") as f:\n",
    "    json.dump(geojson, f, indent=2)\n",
    "\n",
    "# Or print it for inspection\n",
    "print(json.dumps(geojson, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert nexus data gdf to geojson ####\n",
    "\n",
    "filtered_nexus = nexus[nexus[\"id\"] == filtered_gdf.iloc[0][\"toid\"]]\n",
    "\n",
    "# Reproject to WGS84 (4326)\n",
    "filtered_nexus = filtered_nexus.to_crs(\"EPSG:4326\")\n",
    "print(f\"converted CRS -> {filtered_nexus.crs}\\n\")\n",
    "\n",
    "# Create GeoJSON object with structure for ngen.\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"name\": \"nexus_data\",\n",
    "    \"crs\": {\n",
    "        \"type\": \"name\",\n",
    "        \"properties\": {\"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\"},\n",
    "    },\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"id\": row[\"id\"],  # Use the 'id' column as the feature ID\n",
    "            \"properties\": {\n",
    "                \"nexus_type\": row[\"type\"],  # Use the 'type' column\n",
    "                \"toid\": row[\"toid\"],  # Use the 'toid' column\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [\n",
    "                    row.geometry.x,\n",
    "                    row.geometry.y,\n",
    "                ],  # Longitude first, then latitude\n",
    "            },\n",
    "        }\n",
    "        for _, row in filtered_nexus.iterrows()  # Iterate over rows in the GeoDataFrame\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"nexus_data_nex-87405.geojson\", \"w\") as f:\n",
    "    json.dump(geojson, f, indent=2)\n",
    "\n",
    "# Or print it for inspection\n",
    "print(json.dumps(geojson, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gdf.iloc[0].geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load netcdf forcing and attribute files + trim to JRB.\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "attrs_path = r\"C:\\Users\\LeoLo\\Desktop\\attributes.nc\"\n",
    "forc_path = r\"X:\\forcings.nc\"  # \"C:\\Users\\LeoLo\\Desktop\\forcings.nc\"\n",
    "\n",
    "# Open the NetCDF and convert to DataFrame\n",
    "d_a = xr.open_dataset(attrs_path)\n",
    "# attrs = d_a.to_dataframe()\n",
    "\n",
    "d_f = xr.open_dataset(forc_path)\n",
    "# forc = d_f.to_dataframe()\n",
    "\n",
    "# Display the dataset\n",
    "print(d_a)\n",
    "\n",
    "\n",
    "# Get the divide_id coordinate\n",
    "divide_ids = d_a[\"divide_id\"].values\n",
    "\n",
    "# Find duplicate divide_id values\n",
    "unique, counts = np.unique(divide_ids, return_counts=True)\n",
    "duplicates = unique[counts > 1]\n",
    "print(f\"\\n --------\\nAttribute data has {len(duplicates)} duplicate divide_id values.\")\n",
    "\n",
    "\n",
    "# Find duplicate divide_id values\n",
    "divide_ids = d_f[\"divide_id\"].values\n",
    "unique, counts = np.unique(divide_ids, return_counts=True)\n",
    "duplicates = unique[counts > 1]\n",
    "print(f\"\\n --------\\nForcing data has {len(duplicates)} duplicate divide_id values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the divide_ids that are in the JRB, and select the first occurance of any duplicate divide_ids.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## For forcing\n",
    "divide_ids = d_f[\"divide_id\"].values\n",
    "\n",
    "# Find the first occurrence of each divide_id\n",
    "unique_indices = np.unique(divide_ids, return_index=True)[1]\n",
    "first_occurrence_mask = np.zeros_like(divide_ids, dtype=bool)\n",
    "first_occurrence_mask[unique_indices] = True\n",
    "\n",
    "# Apply the mask to the dataset\n",
    "unique_d_f = d_f.isel(divide_id=first_occurrence_mask)\n",
    "\n",
    "# Subset the dataset to include only the desired divide_ids\n",
    "subset_d_f = unique_d_f.sel(divide_id=jrb_divide_ids)\n",
    "\n",
    "\n",
    "## For attributes\n",
    "divide_ids = d_a[\"divide_id\"].values\n",
    "unique_indices = np.unique(divide_ids, return_index=True)[1]\n",
    "first_occurrence_mask = np.zeros_like(divide_ids, dtype=bool)\n",
    "first_occurrence_mask[unique_indices] = True\n",
    "\n",
    "unique_d_a = d_a.isel(divide_id=first_occurrence_mask)\n",
    "subset_d_a = unique_d_a.sel(divide_id=jrb_divide_ids)\n",
    "\n",
    "\n",
    "## Convert to dataframe\n",
    "forc = subset_d_f.to_dataframe()\n",
    "attrs = subset_d_a.to_dataframe()\n",
    "\n",
    "\n",
    "## Trim time to 2000-2005 (divide_id is subindexed by time)\n",
    "# Ensure the second level (time) is a DatetimeIndex\n",
    "forc.index = forc.index.set_levels(pd.to_datetime(forc.index.levels[1]), level=1)\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2005-12-31\"\n",
    "forc = forc.loc[(slice(None), slice(start_date, end_date)), :]\n",
    "\n",
    "# Unstack divide_id so that time is the main index\n",
    "forc_unstacked = forc.unstack(\n",
    "    level=0,\n",
    ")  # Now columns are MultiIndex (divide_id, variable)\n",
    "forc_array = forc_unstacked.to_numpy().reshape(\n",
    "    len(forc_unstacked),\n",
    "    len(forc_unstacked.columns.levels[0]),\n",
    "    -1,\n",
    ")\n",
    "\n",
    "forc_array = np.swapaxes(forc_array, 2, 1)\n",
    "\n",
    "f_xr = subset_d_f.to_array()\n",
    "f_xr = np.swapaxes((np.swapaxes(np.swapaxes(f_xr, 1, 0), 2, 1)), 0, 1)\n",
    "\n",
    "f_xr = f_xr[:2192,]\n",
    "\n",
    "\n",
    "## Save to file\n",
    "forc_path = r\"C:\\Users\\LeoLo\\Desktop\\forcings_jrb\"\n",
    "attrs_path = r\"C:\\Users\\LeoLo\\Desktop\\attributes_jrb\"\n",
    "\n",
    "np.save(forc_path, forc_array)  # (2192, 794, 3)\n",
    "np.save(attrs_path, attrs.to_numpy())  # (794, 28)\n",
    "\n",
    "# save the netcdf files\n",
    "subset_d_a.to_netcdf(r\"C:\\Users\\LeoLo\\Desktop\\attributes_jrb.nc\")\n",
    "subset_d_f.to_netcdf(r\"C:\\Users\\LeoLo\\Desktop\\forcings_jrb.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_d_f[\"time\"][2191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_d_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mod = torch.load(\n",
    "    r\"C:\\Users\\LeoLo\\Desktop\\noaa_owp\\dHBV_2_0\\ngen_files\\data\\dhbv_2_0\\dhbv_merit_conus_100ep.pt\",\n",
    "    map_location=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Other Debug...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get subset of ids for JRB basins ####\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "gdf = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"flowpaths\")\n",
    "nexus = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"nexus\")\n",
    "# Many more layers 'flowpaths', 'divides', 'lakes', 'nexus', 'pois', 'hydrolocations', 'flowpath-attributes',\n",
    "# 'flowpath-attributes-ml', 'network', 'divide-attributes'\n",
    "\n",
    "# print(gdf.head())\n",
    "print(f\"Basins in Juniata RB: {gdf.divide_id} (unique: {gdf.divide_id.nunique()})\")\n",
    "\n",
    "# Select subset of divide_ids\n",
    "jrb_divide_ids = list(gdf.divide_id)[0:1]\n",
    "print(f\"selecting divide_id: {jrb_divide_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(\n",
    "    r\"C:\\Users\\LeoLo\\Desktop\\noaa_owp\\dHBV_2_0\\ngen_resources\\data\\dhbv_2_0\\spatial\\cat-88306.gpkg\",\n",
    "    layer=\"flowpath-attributes-ml\",\n",
    ")\n",
    "# gdf[gdf['divide_id'].isin(jrb_divide_ids)]\n",
    "gdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "path = (\n",
    "    '/gpfs/yxs275/data/hourly/CAMELS_HF/forcing/forcing_1990_2018_gauges_00000_00499.nc'\n",
    ")\n",
    "\n",
    "root = xr.open_dataset(path)\n",
    "\n",
    "root['PET'][:].shape, root['PET'][0, :100].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "path = '/gpfs/yxs275/data/hourly/CAMELS_HF/forcing/forcing_1990_2018_gauges_hourly_00000_00499.nc'\n",
    "\n",
    "zTest_full_time = pd.date_range('2004-10-01 00:00:00', '2018-10-01 00:00:00', freq='h')[\n",
    "    :-1\n",
    "]\n",
    "hourly_x = xr.open_dataset(path).sel(\n",
    "    time=zTest_full_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_x['PET'][0, :25].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Verify hourly dmg matches wencong\n",
    "import xarray as xr\n",
    "\n",
    "dmg_path = '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/hydrodl2/h-dhbv2_3_Qprimeprime_fixed/hourly_simulation_0_00000_00499.nc'\n",
    "hybrid_path = '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/h-dhbv2_3_Qprimeprime_fixed/hourly_simulation_0_00000_00499.nc'\n",
    "wencong_path = '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/wencong_original/h-dhbv2_3_Qprimeprime_fixed/hourly_simulation_0_00000_00499.nc'\n",
    "yalan_path = '/gpfs/yxs275/model_outputs/hourly/distributedHourly/HF_outputs/h-dhbv2_3_Qprimeprime_fixed_aggregated_norm2/hourly_simulation_0_00000_00499.nc'\n",
    "\n",
    "\n",
    "dmg_xr = xr.open_dataset(dmg_path)\n",
    "hybrid_xr = xr.open_dataset(hybrid_path)\n",
    "wencong_xr = xr.open_dataset(wencong_path)\n",
    "yalan_xr = xr.open_dataset(yalan_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmg_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wencong_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yalan_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CAMELS forcing dataset\n",
    "# 100 catchments, 2010-2015\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "forcing_path = '/projects/mhpi/leoglonz/ciroh-ua/dhbv2_mts/ngen_resources/data/forcing/camels_2010-01-01_00_00_00_2011-12-30_23_00_00.nc'\n",
    "\n",
    "forcing_xr = xr.open_dataset(forcing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_xr[\"TMP_2maboveground\"][0, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forcing_xr[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = '/projects/mhpi/leoglonz/ciroh-ua/ciroh-ua-ngen/data/forcing/cats-27_52_67-2015_12_01-2015_12_30.nc'\n",
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['ids'][:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel({'ids': 'cat-27'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ops39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
