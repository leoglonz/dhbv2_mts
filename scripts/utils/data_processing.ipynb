{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get subset of ids for JRB basins ####\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "gdf = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"flowpaths\")\n",
    "nexus = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"nexus\")\n",
    "# Many more layers 'flowpaths', 'divides', 'lakes', 'nexus', 'pois', 'hydrolocations', 'flowpath-attributes',\n",
    "# 'flowpath-attributes-ml', 'network', 'divide-attributes'\n",
    "\n",
    "# print(gdf.head())\n",
    "print(f\"Basins in Juniata RB: {gdf.divide_id} (unique: {gdf.divide_id.nunique()})\")\n",
    "\n",
    "# Select subset of divide_ids\n",
    "jrb_divide_ids = list(gdf.divide_id)[0:1]\n",
    "print(f\"selecting divide_id: {jrb_divide_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"network\")\n",
    "# gdf[gdf['divide_id'].isin(jrb_divide_ids)]\n",
    "gdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CRS projection; need to use the 'flowpaths' layer\n",
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf[gdf[\"divide_id\"].isin(jrb_divide_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert catchment data gdf to geojson ####\n",
    "\n",
    "import json\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "filtered_gdf = gdf[gdf[\"divide_id\"].isin(jrb_divide_ids)]\n",
    "\n",
    "# Reproject to WGS84 (4326)\n",
    "filtered_gdf = filtered_gdf.to_crs(\"EPSG:4326\")\n",
    "print(f\"converted CRS -> {filtered_gdf.crs}\\n\")\n",
    "\n",
    "# Ensure the LINESTRING is closed (first and last points are the same)\n",
    "line = filtered_gdf.iloc[0].geometry\n",
    "if line.coords[0] != line.coords[-1]:\n",
    "    line = Polygon(list(line.coords) + [line.coords[0]])\n",
    "\n",
    "# Update the geometry in the GeoDataFrame\n",
    "filtered_gdf.at[filtered_gdf.index[0], \"geometry\"] = line\n",
    "\n",
    "# Create GeoJSON object with structure for ngen.\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"name\": \"catchment_data\",\n",
    "    \"crs\": {\n",
    "        \"type\": \"name\",\n",
    "        \"properties\": {\"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\"},\n",
    "    },\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"id\": filtered_gdf.iloc[0][\"divide_id\"],  # Use divide_id as the feature ID\n",
    "            \"properties\": {\n",
    "                \"area_sqkm\": filtered_gdf.iloc[0][\n",
    "                    \"areasqkm\"\n",
    "                ],  # Area in square kilometers\n",
    "                \"toid\": filtered_gdf.iloc[0][\"toid\"],  # Related identifier\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": [\n",
    "                    list(line.exterior.coords),\n",
    "                ],  # Extract polygon coordinates\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"catchment_data_cat-88306.geojson\", \"w\") as f:\n",
    "    json.dump(geojson, f, indent=2)\n",
    "\n",
    "# Or print it for inspection\n",
    "print(json.dumps(geojson, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert nexus data gdf to geojson ####\n",
    "\n",
    "filtered_nexus = nexus[nexus[\"id\"] == filtered_gdf.iloc[0][\"toid\"]]\n",
    "\n",
    "# Reproject to WGS84 (4326)\n",
    "filtered_nexus = filtered_nexus.to_crs(\"EPSG:4326\")\n",
    "print(f\"converted CRS -> {filtered_nexus.crs}\\n\")\n",
    "\n",
    "# Create GeoJSON object with structure for ngen.\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"name\": \"nexus_data\",\n",
    "    \"crs\": {\n",
    "        \"type\": \"name\",\n",
    "        \"properties\": {\"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\"},\n",
    "    },\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"id\": row[\"id\"],  # Use the 'id' column as the feature ID\n",
    "            \"properties\": {\n",
    "                \"nexus_type\": row[\"type\"],  # Use the 'type' column\n",
    "                \"toid\": row[\"toid\"],  # Use the 'toid' column\n",
    "            },\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [\n",
    "                    row.geometry.x,\n",
    "                    row.geometry.y,\n",
    "                ],  # Longitude first, then latitude\n",
    "            },\n",
    "        }\n",
    "        for _, row in filtered_nexus.iterrows()  # Iterate over rows in the GeoDataFrame\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save as GeoJSON\n",
    "with open(\"nexus_data_nex-87405.geojson\", \"w\") as f:\n",
    "    json.dump(geojson, f, indent=2)\n",
    "\n",
    "# Or print it for inspection\n",
    "print(json.dumps(geojson, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gdf.iloc[0].geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load netcdf forcing and attribute files + trim to JRB.\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "attrs_path = r\"C:\\Users\\LeoLo\\Desktop\\attributes.nc\"\n",
    "forc_path = r\"X:\\forcings.nc\"  # \"C:\\Users\\LeoLo\\Desktop\\forcings.nc\"\n",
    "\n",
    "# Open the NetCDF and convert to DataFrame\n",
    "d_a = xr.open_dataset(attrs_path)\n",
    "# attrs = d_a.to_dataframe()\n",
    "\n",
    "d_f = xr.open_dataset(forc_path)\n",
    "# forc = d_f.to_dataframe()\n",
    "\n",
    "# Display the dataset\n",
    "print(d_a)\n",
    "\n",
    "\n",
    "# Get the divide_id coordinate\n",
    "divide_ids = d_a[\"divide_id\"].values\n",
    "\n",
    "# Find duplicate divide_id values\n",
    "unique, counts = np.unique(divide_ids, return_counts=True)\n",
    "duplicates = unique[counts > 1]\n",
    "print(f\"\\n --------\\nAttribute data has {len(duplicates)} duplicate divide_id values.\")\n",
    "\n",
    "\n",
    "# Find duplicate divide_id values\n",
    "divide_ids = d_f[\"divide_id\"].values\n",
    "unique, counts = np.unique(divide_ids, return_counts=True)\n",
    "duplicates = unique[counts > 1]\n",
    "print(f\"\\n --------\\nForcing data has {len(duplicates)} duplicate divide_id values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the divide_ids that are in the JRB, and select the first occurance of any duplicate divide_ids.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## For forcing\n",
    "divide_ids = d_f[\"divide_id\"].values\n",
    "\n",
    "# Find the first occurrence of each divide_id\n",
    "unique_indices = np.unique(divide_ids, return_index=True)[1]\n",
    "first_occurrence_mask = np.zeros_like(divide_ids, dtype=bool)\n",
    "first_occurrence_mask[unique_indices] = True\n",
    "\n",
    "# Apply the mask to the dataset\n",
    "unique_d_f = d_f.isel(divide_id=first_occurrence_mask)\n",
    "\n",
    "# Subset the dataset to include only the desired divide_ids\n",
    "subset_d_f = unique_d_f.sel(divide_id=jrb_divide_ids)\n",
    "\n",
    "\n",
    "## For attributes\n",
    "divide_ids = d_a[\"divide_id\"].values\n",
    "unique_indices = np.unique(divide_ids, return_index=True)[1]\n",
    "first_occurrence_mask = np.zeros_like(divide_ids, dtype=bool)\n",
    "first_occurrence_mask[unique_indices] = True\n",
    "\n",
    "unique_d_a = d_a.isel(divide_id=first_occurrence_mask)\n",
    "subset_d_a = unique_d_a.sel(divide_id=jrb_divide_ids)\n",
    "\n",
    "\n",
    "## Convert to dataframe\n",
    "forc = subset_d_f.to_dataframe()\n",
    "attrs = subset_d_a.to_dataframe()\n",
    "\n",
    "\n",
    "## Trim time to 2000-2005 (divide_id is subindexed by time)\n",
    "# Ensure the second level (time) is a DatetimeIndex\n",
    "forc.index = forc.index.set_levels(pd.to_datetime(forc.index.levels[1]), level=1)\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2005-12-31\"\n",
    "forc = forc.loc[(slice(None), slice(start_date, end_date)), :]\n",
    "\n",
    "# Unstack divide_id so that time is the main index\n",
    "forc_unstacked = forc.unstack(\n",
    "    level=0,\n",
    ")  # Now columns are MultiIndex (divide_id, variable)\n",
    "forc_array = forc_unstacked.to_numpy().reshape(\n",
    "    len(forc_unstacked),\n",
    "    len(forc_unstacked.columns.levels[0]),\n",
    "    -1,\n",
    ")\n",
    "\n",
    "forc_array = np.swapaxes(forc_array, 2, 1)\n",
    "\n",
    "f_xr = subset_d_f.to_array()\n",
    "f_xr = np.swapaxes((np.swapaxes(np.swapaxes(f_xr, 1, 0), 2, 1)), 0, 1)\n",
    "\n",
    "f_xr = f_xr[:2192,]\n",
    "\n",
    "\n",
    "## Save to file\n",
    "forc_path = r\"C:\\Users\\LeoLo\\Desktop\\forcings_jrb\"\n",
    "attrs_path = r\"C:\\Users\\LeoLo\\Desktop\\attributes_jrb\"\n",
    "\n",
    "np.save(forc_path, forc_array)  # (2192, 794, 3)\n",
    "np.save(attrs_path, attrs.to_numpy())  # (794, 28)\n",
    "\n",
    "# save the netcdf files\n",
    "subset_d_a.to_netcdf(r\"C:\\Users\\LeoLo\\Desktop\\attributes_jrb.nc\")\n",
    "subset_d_f.to_netcdf(r\"C:\\Users\\LeoLo\\Desktop\\forcings_jrb.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_d_f[\"time\"][2191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_d_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mod = torch.load(\n",
    "    r\"C:\\Users\\LeoLo\\Desktop\\noaa_owp\\dHBV_2_0\\ngen_files\\data\\dhbv_2_0\\dhbv_merit_conus_100ep.pt\",\n",
    "    map_location=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Other Debug...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get subset of ids for JRB basins ####\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "gdf = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"flowpaths\")\n",
    "nexus = gpd.read_file(r\"C:\\Users\\LeoLo\\Desktop\\jrb\\jrb_2.gpkg\", layer=\"nexus\")\n",
    "# Many more layers 'flowpaths', 'divides', 'lakes', 'nexus', 'pois', 'hydrolocations', 'flowpath-attributes',\n",
    "# 'flowpath-attributes-ml', 'network', 'divide-attributes'\n",
    "\n",
    "# print(gdf.head())\n",
    "print(f\"Basins in Juniata RB: {gdf.divide_id} (unique: {gdf.divide_id.nunique()})\")\n",
    "\n",
    "# Select subset of divide_ids\n",
    "jrb_divide_ids = list(gdf.divide_id)[0:1]\n",
    "print(f\"selecting divide_id: {jrb_divide_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(\n",
    "    r\"C:\\Users\\LeoLo\\Desktop\\noaa_owp\\dHBV_2_0\\ngen_resources\\data\\dhbv_2_0\\spatial\\cat-88306.gpkg\",\n",
    "    layer=\"flowpath-attributes-ml\",\n",
    ")\n",
    "# gdf[gdf['divide_id'].isin(jrb_divide_ids)]\n",
    "gdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "path = (\n",
    "    '/gpfs/yxs275/data/hourly/CAMELS_HF/forcing/forcing_1990_2018_gauges_00000_00499.nc'\n",
    ")\n",
    "\n",
    "root = xr.open_dataset(path)\n",
    "\n",
    "root['PET'][:].shape, root['PET'][0, :100].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "path = '/gpfs/yxs275/data/hourly/CAMELS_HF/forcing/forcing_1990_2018_gauges_hourly_00000_00499.nc'\n",
    "\n",
    "zTest_full_time = pd.date_range('2004-10-01 00:00:00', '2018-10-01 00:00:00', freq='h')[\n",
    "    :-1\n",
    "]\n",
    "hourly_x = xr.open_dataset(path).sel(\n",
    "    time=zTest_full_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_x['PET'][0, :25].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Verify hourly dmg matches wencong\n",
    "import xarray as xr\n",
    "\n",
    "dmg_path = '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/hydrodl2/h-dhbv2_3_Qprimeprime_fixed/hourly_simulation_0_00000_00499.nc'\n",
    "hybrid_path = '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/h-dhbv2_3_Qprimeprime_fixed/hourly_simulation_0_00000_00499.nc'\n",
    "wencong_path = '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/wencong_original/h-dhbv2_3_Qprimeprime_fixed/hourly_simulation_0_00000_00499.nc'\n",
    "yalan_path = '/gpfs/yxs275/model_outputs/hourly/distributedHourly/HF_outputs/h-dhbv2_3_Qprimeprime_fixed_aggregated_norm2/hourly_simulation_0_00000_00499.nc'\n",
    "\n",
    "\n",
    "dmg_xr = xr.open_dataset(dmg_path)\n",
    "hybrid_xr = xr.open_dataset(hybrid_path)\n",
    "wencong_xr = xr.open_dataset(wencong_path)\n",
    "yalan_xr = xr.open_dataset(yalan_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmg_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wencong_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yalan_xr['Simulation'][0, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CAMELS forcing dataset\n",
    "# 100 catchments, 2010-2015\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "example_path = '/projects/mhpi/leoglonz/ciroh-ua/ciroh-ua-ngen/data/forcing/cats-27_52_67-2015_12_01-2015_12_30.nc'\n",
    "\n",
    "forcing_xr = xr.open_dataset(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking\n",
    "\n",
    "import torch\n",
    "\n",
    "bench_qs = torch.load(\n",
    "    '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/cat-2543/benchmark_qs.pt',\n",
    "    weights_only=False,\n",
    ").squeeze()\n",
    "ngen_qs = (\n",
    "    torch.load(\n",
    "        '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/cat-2543/ngen_qs.pt',\n",
    "        weights_only=False,\n",
    "    )[8592:]\n",
    "    .squeeze()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "print(bench_qs.shape, ngen_qs.shape)\n",
    "maxl = min(bench_qs.shape[0], ngen_qs.shape[0])\n",
    "\n",
    "# maxl = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_qs = bench_qs[:maxl]\n",
    "ngen_qs = ngen_qs[:maxl]\n",
    "\n",
    "bench_qs.shape, ngen_qs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(ngen_qs - bench_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot simulations for first 7 days\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(bench_qs, label='Benchmark Qs')\n",
    "plt.plot(ngen_qs, label='NGen Qs')\n",
    "plt.legend()\n",
    "title = f'Cat-2543 NGen vs Benchmark Runoff (2009-01-01 to 2015-12-30)\\nMax Abs Diff: {max(ngen_qs - bench_qs):.4f} m3/s'\n",
    "plt.title(title)\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Streamflow (m3/s)')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building hourly CAMELS forcing dataset.\n",
    "3 catchments, 2010-2012.\n",
    "\n",
    "Mirrors structure of ./ngen/data/forcing/cat-67_2015-12-01 00_00_00_2015-12-30 23_00_00.csv\n",
    "\n",
    "@leoglonz\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "n_cat = 3\n",
    "t_start = '2008-01-09 00:00:00'\n",
    "t_end = '2010-12-30 23:00:00'\n",
    "\n",
    "example_path = '/projects/mhpi/leoglonz/ciroh-ua/ciroh-ua-ngen/data/forcing/cat-67_2015-12-01 00_00_00_2015-12-30 23_00_00.csv'\n",
    "camels_path = '/gpfs/yxs275/data/hourly/CAMELS_HF/forcing/forcing_1990_2018_gauges_hourly_00000_00499.nc'\n",
    "out_path = f'/projects/mhpi/leoglonz/ciroh-ua/dhbv2_mts/ngen_resources/data/forcing/camels_{t_start.replace(\":\", \"_\").replace(\" \", \"_\")}_{t_end.replace(\":\", \"_\").replace(\" \", \"_\")}.nc'\n",
    "\n",
    "df = pd.read_csv(example_path)\n",
    "camels_xr = xr.open_dataset(camels_path)\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "def transform_dataset(ds_in):\n",
    "    \"\"\"Transform CAMELS dataset to match target structure.\"\"\"\n",
    "    # 1. Rename Dimensions first\n",
    "    ds = ds_in.rename({'gauge': 'catchment-id'})\n",
    "\n",
    "    # --- INSERT SUBSETTING HERE (Before creating new vars or dropping coords) ---\n",
    "    # Use .isel (index select) for the first 100 catchments\n",
    "    # Use .sel (label select) for the specific date range\n",
    "    ds = ds.isel({'catchment-id': slice(498, 499)})\n",
    "    ds = ds.sel(time=slice(t_start, t_end))\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    # 2. Rename Existing Variables\n",
    "    ds = ds.rename(\n",
    "        {\n",
    "            'P': 'precip_rate',\n",
    "            'T': 'TMP_2maboveground',\n",
    "            'PET': 'PET_hargreaves',\n",
    "        },\n",
    "    )\n",
    "\n",
    "    for var in ds.variables:\n",
    "        ds[var].encoding = {}\n",
    "\n",
    "    # 3. Create 'ids' Variable\n",
    "    raw_ids = ds['catchment-id'].values.astype(str)\n",
    "\n",
    "    # Prepend \"cat-\" using numpy's string operations\n",
    "    # cat_ids will be like [\"cat-2453\", \"cat-2454\", ...]\n",
    "    cat_ids = np.char.add('cat-', raw_ids)\n",
    "\n",
    "    ds['ids'] = (('catchment-id',), cat_ids)\n",
    "\n",
    "    # 4. Create 'Time' Variable with Nanosecond Units\n",
    "    # .view('int64') accesses the raw nanoseconds from the datetime64[ns] object\n",
    "    # .astype('float64') converts that integer count to a float\n",
    "    time_values = ds['time'].values.view('int64').astype('float64')\n",
    "\n",
    "    time_broadcasted = np.tile(\n",
    "        time_values,\n",
    "        (ds.sizes['catchment-id'], 1),\n",
    "    )\n",
    "\n",
    "    ds['Time'] = (('catchment-id', 'time'), time_broadcasted)\n",
    "    ds['Time'].attrs = {'units': 'ns'}\n",
    "\n",
    "    # 5. Create Zero-Filled Variables\n",
    "    zero_vars = [\n",
    "        'APCP_surface',\n",
    "        'DLWRF_surface',\n",
    "        'DSWRF_surface',\n",
    "        'PRES_surface',\n",
    "        'SPFH_2maboveground',\n",
    "        'UGRD_10maboveground',\n",
    "        'VGRD_10maboveground',\n",
    "    ]\n",
    "\n",
    "    # Create zeros based on the new subset shape\n",
    "    shape = (ds.sizes['catchment-id'], ds.sizes['time'])\n",
    "    zeros = np.zeros(shape, dtype='float64')\n",
    "\n",
    "    for var in zero_vars:\n",
    "        ds[var] = (('catchment-id', 'time'), zeros)\n",
    "        ds[var].encoding = {'_FillValue': None}  # Ensure no default fill values exist\n",
    "\n",
    "    # Convert to temp to kelvin\n",
    "    ds['TMP_2maboveground'] = ds['TMP_2maboveground'].astype('float64') + np.float64(\n",
    "        273.15,\n",
    "    )\n",
    "    ds['TMP_2maboveground'].attrs['units'] = 'K'\n",
    "    ds['precip_rate'].attrs['units'] = 'mm hr-1'\n",
    "    ds['PET_hargreaves'].attrs['units'] = 'mm hr-1'\n",
    "    ds['APCP_surface'].attrs['units'] = 'kg m-2'\n",
    "    ds['DLWRF_surface'].attrs['units'] = 'W m-2'\n",
    "    ds['DSWRF_surface'].attrs['units'] = 'W m-2'\n",
    "    ds['PRES_surface'].attrs['units'] = 'Pa'\n",
    "    ds['SPFH_2maboveground'].attrs['units'] = 'g g-1'\n",
    "    ds['UGRD_10maboveground'].attrs['units'] = 'm s-1'\n",
    "    ds['VGRD_10maboveground'].attrs['units'] = 'm s-1'\n",
    "\n",
    "    ds['TMP_2maboveground'].attrs['units'] = 'K'\n",
    "    ds['precip_rate'].attrs['units'] = 'mm hr-1'\n",
    "    ds['PET_hargreaves'].attrs['units'] = 'mm hr-1'\n",
    "\n",
    "    ds['TMP_2maboveground'].encoding = {'dtype': 'float64'}\n",
    "\n",
    "    # 6. NOW it is safe to drop the coordinates\n",
    "    ds = ds.drop_vars(['time', 'catchment-id'])\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "formatted_ds = transform_dataset(camels_xr)\n",
    "\n",
    "# save to nc\n",
    "formatted_ds.to_netcdf(\n",
    "    out_path,\n",
    "    format='NETCDF4',\n",
    "    engine='netcdf4',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "bench = torch.load(\n",
    "    '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/cat-16867/bench_2008-2012_cat-16867.pt',\n",
    ").squeeze()\n",
    "ngen = torch.load(\n",
    "    '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/cat-16867/ngen_qs.pt',\n",
    ")\n",
    "\n",
    "torch.save(\n",
    "    ngen[:336],\n",
    "    '/projects/mhpi/leoglonz/ciroh-ua/dmg/hf_outputs/cat-16867/ngen_qs_14d.pt',\n",
    ")\n",
    "bench.shape, ngen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bench - ngen[:336]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bench - ngen[:336]).max()\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(bench, label='Benchmark Qs')\n",
    "plt.plot(ngen[:336], label='NGen Qs')\n",
    "plt.legend()\n",
    "title = 'Cat-16867 NGen vs Benchmark Runoff'\n",
    "plt.title(title)\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Streamflow (m3/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "path = '/Users/leoglonz/Desktop/dhbv2_mts/ngen_resources/data/forcing/camels_2008-01-09_00_00_00_2015-12-30_23_00_00.nc'\n",
    "\n",
    "f=xr.open_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat-2453', 'cat-2454', 'cat-2455'], dtype='<U8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['ids'][:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
